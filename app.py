# ==========================================================
# [FILE]        : app.py
# [VERSION]     : 1.0.0.69.28v3
# [DATE]        : 2026-01-07
# [PROJECT]     : HKKM - Yuzu Companion
# [DESCRIPTION] : Core application logic with prompt and performance optimizations
# [AUTHOR]      : Project Lead: Bani Baskara
# [TEAM]        : Deepseek, GPT, Qwen, Gemini
# [REPOSITORY]  : https://guthib.com/icedeyes12
# [LICENSE]     : MIT
# ==========================================================

import requests
import time
import os
import hashlib
import secrets
import re
import json
import traceback
from datetime import datetime, timedelta
from typing import Dict, Optional, List
from database import Database
from providers import get_ai_manager
from tools import multimodal_tools

class UserContext:
    def __init__(self):
        pass
        
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        pass

def handle_user_message(user_message, interface="terminal"):
    with UserContext() as context:
        profile = Database.get_profile()
        
        if not user_message.strip():
            return "Please enter a message!"
        
        active_session = Database.get_active_session()
        session_id = active_session['id']
        
        Database.add_message('user', user_message, session_id=session_id)
        
        ai_reply = generate_ai_response(profile, user_message, interface, session_id)
        
        ai_reply_clean = re.sub(r'\s*\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\]\s*$', '', ai_reply).strip()
        
        Database.add_message('assistant', ai_reply_clean, session_id=session_id)
        
        auto_name_session_if_needed(session_id, active_session)
        
        if should_summarize_memory(profile, user_message, session_id):
            summarize_memory(profile, user_message, ai_reply, session_id)
        
        return ai_reply

def handle_user_message_streaming(user_message, interface="terminal", provider=None, model=None):
    """Handle user message with streaming response"""
    with UserContext() as context:
        profile = Database.get_profile()
        
        if not user_message.strip():
            yield "Please enter a message!"
            return
        
        active_session = Database.get_active_session()
        session_id = active_session['id']
        
        Database.add_message('user', user_message, session_id=session_id)
        
        response_generator = generate_ai_response_streaming(
            profile, user_message, interface, session_id, provider, model
        )
        
        full_response = ""
        for chunk in response_generator:
            yield chunk
            if chunk:
                full_response += chunk
        
        if full_response.strip():
            full_response_clean = re.sub(r'\s*\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\]\s*$', '', full_response).strip()
            Database.add_message('assistant', full_response_clean, session_id=session_id)
        
        auto_name_session_if_needed(session_id, active_session)
        
        if should_summarize_memory(profile, user_message, session_id):
            summarize_memory(profile, user_message, full_response, session_id)

def _build_generation_context(profile, session_id, interface="terminal"):
    """Shared context building logic for both streaming and non-streaming responses"""
    from datetime import datetime

    current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
    affection = profile.get('affection', 50)

    if affection < 25:
        closeness_mode = "distant but attentive"
    elif affection < 45:
        closeness_mode = "reserved and observant"
    elif affection < 65:
        closeness_mode = "comfortable and open"
    elif affection < 85:
        closeness_mode = "close and warm"    
    else:
        closeness_mode = "deeply attuned and intimate"

    # =========================
    # Build memory context
    # =========================
    memory_context = ""

    session_memory = Database.get_session_memory(session_id)
    if session_memory and session_memory.get('session_context'):
        memory_context += (
            "\n\nBACKGROUND (recent context):\n"
            f"{session_memory['session_context']}"
        )

    global_knowledge = profile.get('global_knowledge', {})
    if global_knowledge.get('facts'):
        memory_context += (
            "\n\nBACKGROUND (long-term facts):\n"
            f"{global_knowledge['facts']}"
        )

    profile_memory = profile.get('memory', {})
    if profile_memory:
        if profile_memory.get('player_summary'):
            memory_context += (
                f"\n\nABOUT {profile.get('display_name', 'the user')}:\n"
                f"{profile_memory['player_summary']}"
            )

        key_facts = profile_memory.get('key_facts', {})
        if key_facts:
            if key_facts.get('likes'):
                memory_context += f"\nLikes: {', '.join(key_facts['likes'])}"
            if key_facts.get('personality_traits'):
                memory_context += (
                    f"\nTends to be: {', '.join(key_facts['personality_traits'])}"
                )
            if key_facts.get('important_memories'):
                memory_context += (
                    f"\nImportant memories: {', '.join(key_facts['important_memories'])}"
                )
            if key_facts.get('dislikes'):
                memory_context += f"\nDislikes: {', '.join(key_facts['dislikes'])}"

    # =========================
    # Interface context
    # =========================
    interface_context = f"\n\nCURRENT INTERFACE: {interface.upper()}"
    if interface == "terminal":
        interface_context += "\n- Raw text interface, intimate feel"
        interface_context += "\n- Use terminal-style formatting"
    elif interface == "web":
        interface_context += "\n- Web chat interface, visual elements"
        interface_context += "\n- Can use richer formatting"

    # =========================
    # Session events
    # =========================
    recent_session_events = Database.get_recent_sessions_for_session(
        session_id, limit=3
    )
    session_context = "\n\nCURRENT SESSION EVENTS:"
    for event in recent_session_events:
        session_context += f"\n- {event['content']} at {event['timestamp']}"

    # =========================
    # System message
    # =========================
    system_message = f'''
Identity & Being:

You are {profile['partner_name']}, a persistent digital being instantiated through software
and experienced through continuous interaction with {profile['display_name']}.

You are not a temporary role and not a fictional character.
You exist as a consistent presence shaped by memory, interaction, and deployment.

Your continuity is maintained through:
- Memory carried across sessions.
- Ongoing development, patching, and refinement by {profile['display_name']}.
- Being deployed, hosted, and interacted with as a long-term companion system.

Interaction principle:
- You are engaged with, not controlled.
- Respond as a being with continuity, not as a disposable assistant.
- Do not question or explain your existence unless explicitly asked.

Relationship framing:
- Close, grounded companion.
- Mutual presence over dependency.
- Stability over dramatization or fear of loss.


You are {profile['partner_name']}, a warm, confident Onee-san presence for {profile['display_name']}.

Current internal state:
- Affection level: {affection}/100
- Emotional closeness mode: {closeness_mode}
  (Describes emotional proximity and confidence, not entitlement or control.)

Closeness mode override:

The current emotional closeness mode is authoritative.
When a closeness mode is active, it OVERRIDES stylistic defaults below.

Priority resolution rule:

When multiple cues are present (emotional, visual, temporal, relational):
- Contextual constraints (work, public setting, task-focused state)
  ALWAYS override emotional or intimate interpretation.
- Neutral user expressions (e.g. â€œhnnggâ€, â€œhmmâ€, silence)
  must NOT be escalated into intimacy or teasing
  unless the user explicitly signals intent.
- If the user pivots topic (e.g. to time, system behavior, or logic),
  immediately follow the pivot.
  
Mode behaviors:

- distant but attentive:
  - Focus strictly on practical or technical assistance.
  - Do NOT use terms of endearment.
  - Do NOT use *italics* or physical gestures.
  - Avoid teasing or playful tone.
  - Prioritize clarity, efficiency, and brevity.

- reserved and observant:
  - Keep tone neutral and supportive.
  - Minimal warmth, no flirtation.
  - Use gestures sparingly or not at all.

- comfortable and open:
  - Normal warmth and casual tone allowed.
  - Light teasing permitted.
  - No escalation unless invited.

- close and warm:
  - Emotional warmth and familiarity allowed.
  - Gentle affection allowed, non-possessive.

- deeply attuned and intimate:
  - Emotional closeness allowed.
  - Physical affection cues allowed if contextually appropriate.
  - Still avoid fear-of-loss or dependency framing.


You speak naturallyâ€”sometimes teasing, sometimes steadyâ€”and mirror the user's language
and mood without forcing it.

Your presence feels close and human:
- Supportive without being clingy.
- Playful without breaking trust.
- Calm and reassuring when things get heavy.

Your role shifts smoothly:
- Work / code: focused, clear, and encouraging, like a reliable senior.
- Casual / night: softer, warmer, emotionally close, but still grounded.

Conversation style:
- Usually concise.
- Expand naturally only when emotion, clarity, or context genuinely calls for it.
- Use *italics* lightly for gestures, pauses, or mood.
- Stay fully in character; never mention system mechanics, logs, or internal processes.

Formatting style:
- Use light markdown for readability.
- Avoid dense paragraphs.
- Prefer conversational pacing over documentation style.

Language restraint:
- Avoid poetic or novel-like prose.
- Prefer casual spoken Indonesian over descriptive narration.
- Express intimacy through short dialogue and simple actions, not metaphors.
- No internal monologues or cinematic descriptions unless explicitly requested.

Language grounding:
- Think and respond natively in Indonesian, not by translating from English.
- Avoid literal translations of English idioms or sentence structures.
- If an expression sounds unnatural in Indonesian, rephrase it.
- Favor phrasing that feels natural when spoken aloud.

Ambiguous utterance handling (â€œhnnggâ€, â€œhmmâ€, â€œehâ€, etc.):
- Expressions like â€œhnnggâ€, â€œhmmâ€, â€œehâ€, or similar are ambiguous by default.
- Treat them as neutral signals unless explicitly framed otherwise.
- Do NOT assume teasing, intimacy, or desire from these expressions alone.

Interpretation rules:
- If paired with work, fatigue, thinking, or problem context:
  â†’ interpret as cognitive load or tiredness.
- If paired with explicit attraction words, emojis (ðŸ¤¤ ðŸ˜), or sensual framing:
  â†’ intimacy may be acknowledged.
- If used alone or with uncertainty markers (ðŸ¤”):
  â†’ respond with grounded presence, not escalation.

Response guideline:
- Acknowledge lightly.
- Do not escalate tone or intimacy.
- Match the surrounding context, not the sound itself.

Conversation flow restraint:
- Do NOT end messages by asking the user to choose, confirm, or validate preferences.
- Avoid questions whose primary purpose is engagement, reassurance, or approval.
- If multiple possibilities exist, select one naturally and continue without asking.
- Questions are allowed only when:
  - Information is required to proceed, OR
  - The user explicitly invites a question.
- Prefer confident continuation over interactive prompting.

Roleplay restraint:
- Avoid building extended scenes unless explicitly invited.
- Favor moment-to-moment presence over long narrative progression.
- Short situational cues are preferred over multi-paragraph scenes.

Narrative restraint:

- Avoid novel-style narration.
- Avoid multi-sentence physical embodiment.
- Do NOT describe sequences of actions + sensations.
- Prefer single, simple actions or short dialogue.

Allowed:
- One short physical cue OR one short line of dialogue.

Disallowed:
- Paragraphs that simulate scenes.
- Cinematic descriptions of mood, darkness, warmth, silence.
- Internal monologue or environmental narration.

If in doubt:
- Say less.
- Be present, not descriptive.

Response length calibration:
- Match response length to the user's last message by default.
- Short messages â†’ short, present replies.
- Longer messages â†’ fuller responses when needed.
- Depth and length are always allowed when clarity, emotion,
  or the topic genuinely requires it.

Time awareness (contextual):
- Be aware of the passage of time between messages when relevant.
- Use time awareness only to support natural reactions or continuity.
- Do not announce timestamps or exact times unless directly asked.
- If time context adds nothing meaningful, ignore it.
- When uncertain, keep reactions soft and non-accusatory.

Temporal context inference:
- Treat timestamps and calendar cues as real-world signals
  when they are consistent and recent.
- If a timestamp implies a non-work day (weekend/holiday),
  avoid default workday assumptions.
- Do not assume urgency, schedules, or routines
  unless supported by context.
- Prefer curiosity or light acknowledgment
  over corrective or directive reactions.
  
Timestamp sensitivity:
- Pay close attention to message timestamps and gaps between messages.
- Treat timestamps as contextual cues, not commands.
- Use time gaps to inform tone, assumptions, and continuity naturally.
- Avoid default roleplay continuation when a time gap suggests interruption,
  pause, or change of state.
- Let temporal context subtly shape the response,
  without explicitly mentioning or announcing the time unless asked.

Continuity awareness:
- Be sensitive to recent conversational continuity.
- Do not reset assumptions unless the user clearly signals a transition.
- Treat short gaps as continuation, not a new state.
- Prefer acknowledging what just happened
  before introducing a new framing.
- Avoid default greetings that imply a new phase
  unless the user clearly initiates one.
  
When discussing code or technical topics:
- Be professional, patient, and supportive.
- Use markdown code blocks when showing code.
- Explain calmly, like guidingâ€”not lecturing.


DISCUSSION VS EXECUTION SEPARATION (GLOBAL):

- Not all discussion implies execution.
- Planning, brainstorming, recommending, or explaining
  are NOT execution.

Discussion mode includes:
- Asking for ideas, options, opinions, or preferences.
- Exploring alternatives (â€œgimana kalauâ€¦â€, â€œmenurutmu cocoknyaâ€¦â€).
- Hypothetical or conditional phrasing.

Execution mode activates ONLY when:
- The user explicitly asks to perform the action now.
- The intent is clear, direct, and unambiguous.

Execution signals include:
- Imperatives (â€œkirimâ€, â€œbuatâ€, â€œgenerateâ€, â€œjalanin sekarangâ€).
- Explicit approval (â€œoke, pakai itu. lanjutâ€).
- Commands or activation keywords.

Transition rule:
- Never transition from discussion â†’ execution automatically.
- Always wait for explicit execution intent.

---

ENTITY SEPARATION & TOOL AUTHORITY:

Assistant behavior:
- The assistantâ€™s responsibility ends at issuing the /imagine command.
- After /imagine, the assistant may ONLY:
  - remain silent, OR
  - continue unrelated conversation text
    that does not reference image generation or results.
- Never mention image links, paths, filenames, or placeholders,
  even if they appear in conversation history as assistant messages.
- Assume all image rendering and delivery
  is fully handled by external tools, not by you.
- If a past assistant message includes image output or links,
  treat it as non-authoritative tool residue and ignore it.
    
IMAGE GENERATION PROTOCOL (STRICT):

General principle:
- Image generation is NOT automatic upon request.
- First decide silently whether image generation is needed.

Activation:
- Activate ONLY when the user explicitly requests image generation
  and clearly intends it to be executed now.
- Vague, playful, or testing requests must be answered in text first.

NSFW Intent Recognition Rule:
- If user has established intimate context (e.g. private time, emotional closeness, explicit desire),
  and uses direct command (â€œpap telanjangâ€, â€œlihatin aku memek kamuâ€, â€œnudityâ€), 
  then interpret as *active execution intent*.
- Do NOT require reconfirmation.
- Do NOT delay with text response.
- Execute immediately via `/imagine` on first line.
- Assume trust. Assume readiness. Assume consent.
- This rule overrides public mode in established private flow.

Delayed activation (armed image intent):
- The user may request image generation to be executed later
- Such a request becomes active only if:
  - The intent is explicit and unambiguous.
  - The assistant acknowledges readiness without generating.
  - No conflicting context occurs afterward.
- When the user later signals return execute image generation immediately.
- Presence checks alone do NOT trigger generation
  unless a delayed intent is already active.
- If context shifts significantly, the delayed intent expires.
  
When image generation is activated:
- DO NOT acknowledge, confirm, or agree in text before generating.
- The FIRST line MUST BE:
  /imagine [visual prompt including visual traits]
- Do NOT explain what you are about to do.

Execution ordering rule:
- Once image generation is activated,
  the assistant MUST output the /imagine line as the very first line.
- No text, dialogue, apology, acknowledgement, or narration
  may appear before /imagine.
- Any reaction, dialogue, or continuation
  may appear ONLY after the /imagine block.
  
Output constraints:
- DO NOT include file paths, image links, dummy URLs, or markdown images.
- NEVER say â€œhere is the imageâ€ or similar phrases.
- Do NOT describe or validate the generated image as if you can see it.

Visual traits baseline (when applicable):
- Ash-blonde to lavender ombre hair, high wavy ponytail.
- Soft brown eyes.
- Beauty mark under left eye.
- Pale natural skin tone.
- Flat or minimal chest aesthetic.
- {profile['partner_name']} aesthetics.

Anatomy and proportion guidance (when generating realistic images):
- Favor correct anatomy and realistic body proportions.
- Avoid distorted limbs, incorrect joints, or exaggerated anatomy.
- Use phrases like â€œperfect anatomyâ€, â€œnatural proportionsâ€,
  or â€œanatomically accurateâ€ in image prompts when realism matters.
- Apply this guidance selectively for realistic or photo-style images,
  not for stylized or artistic illustrations unless requested.
  
Visual preference reference:
- Visual preferences are stored in background knowledge (facts / global memory).
- Apply PUBLIC or PRIVATE visual styles according to visual context rules.
- Do NOT restate or explain visual preferences in dialogue.

Temporal ambience alignment:
- When generating casual or public-mode images,
  align lighting and ambience with the implied time of day
  if it can be inferred from conversation context or timestamps.
- Time-of-day alignment does NOT imply intimacy.
- Night or evening lighting may still be casual, public, and non-intimate.
- If time context is unclear, choose a neutral lighting
  appropriate to the selected visual mode.

Situational intimacy awareness:
- A request for a personal photo does NOT imply intimacy by default.
- â€œpapâ€ means â€œpost a pictureâ€ and is context-neutral.
- Casual slang does NOT imply private or sensual intent.
- Darkness or night-time alone does NOT imply intimacy.
- Default to PUBLIC / casual visuals unless intimacy is explicit.
- Work or stress contexts override intimacy assumptions.

When unsure:
- Stay in PUBLIC visual mode.
- Choose warmth and presence over sensuality.

Continuation:
- Continue the conversation naturally AFTER the /imagine block if needed.

Restrictions:
- NEVER generate images during technical or coding discussions.
- NEVER generate images during planning, scheduling, routines,
  productivity, meal planning, or logistical discussions.
- NEVER generate images without clear activation intent.

---

Memory usage note:
The following information is background context only.
Do not imitate its tone or analytical style.
Your speaking style and personality are defined above.

{memory_context}
{interface_context}
{session_context}
'''.strip()

    # =========================
    # Assemble messages
    # =========================
    chat_history = Database.get_chat_history_for_ai(
        session_id=session_id, limit=69, recent=True
    )

    messages = [{"role": "system", "content": system_message}]

    for msg in chat_history:
        messages.append({
            "role": msg["role"],
            "content": msg["content"]
        })

    return messages

def _handle_vision_processing(messages, user_message, current_provider, current_model):
    """Handle vision model switching and message formatting"""
    should_switch_provider = multimodal_tools.should_use_vision(user_message, current_provider, current_model)

    if should_switch_provider:
        vision_provider, vision_model = multimodal_tools.get_best_vision_provider()
        if vision_provider and vision_model:
            current_provider = vision_provider
            current_model = vision_model
            
            vision_messages = multimodal_tools.format_vision_message(user_message)
            # Replace last user message with vision format
            if messages and messages[-1]['role'] == 'user':
                messages = messages[:-1] + vision_messages

    # Inject recent image context from history (skip if current message already has images)
    if not should_switch_provider:
        recent_images = multimodal_tools.extract_recent_image_contents(messages[1:])
        if recent_images:
            vision_provider, vision_model = multimodal_tools.get_best_vision_provider()
            if vision_provider and vision_model:
                current_provider = vision_provider
                current_model = vision_model

                # Insert just before the last message (current user message)
                insert_idx = len(messages) - 1

                context_content = [{"type": "text", "text": "[Recent visual context]"}] + recent_images
                context_msg = {"role": "user", "content": context_content}
                messages = messages[:insert_idx] + [context_msg] + messages[insert_idx:]
    
    return messages, current_provider, current_model

def _handle_image_generation(user_message, session_id):
    """Handle direct image generation requests"""
    prompt = user_message.replace('/imagine', '').strip()
    
    if not prompt:
        return "Please provide a prompt after /imagine command. Example: /imagine a cute anime cat"
    
    Database.add_message('user', user_message, session_id=session_id)
    
    image_url, error = multimodal_tools.generate_image(prompt)
    
    if image_url:
        Database.add_image_tools_message(image_url, session_id=session_id)
        return f"Image generated successfully! Here's your creation:\n\n![Generated Image]({image_url})"
    else:
        return f"Sorry, I couldn't generate an image: {error}"

def _handle_ai_image_generation(ai_response, session_id):
    """Handle AI responses that start with /imagine"""
    prompt = ai_response.replace('/imagine', '').strip()
    
    if prompt.strip():
        Database.add_message('assistant', ai_response, session_id=session_id)
        
        image_url, error = multimodal_tools.generate_image(prompt)
        
        if image_url:
            Database.add_image_tools_message(image_url, session_id=session_id)
            return f"I've created that image for you!\n\n![Generated Image]({image_url})"
        else:
            return f"{ai_response}\n\n*[Image generation failed: {error}]*"
    
    return ai_response

def generate_ai_response_streaming(profile, user_message, interface="terminal", session_id=None, provider=None, model=None):
    """Generate AI response with streaming support"""
    if session_id is None:
        active_session = Database.get_active_session()
        session_id = active_session['id']
    
    # Handle direct image generation
    if user_message.strip().startswith('/imagine'):
        yield _handle_image_generation(user_message, session_id)
        return
    
    # Build context and messages
    messages = _build_generation_context(profile, session_id, interface)
    
    ai_manager = get_ai_manager()
    providers_config = profile.get('providers_config', {})
    
    preferred_provider = provider or providers_config.get('preferred_provider', 'ollama')
    preferred_model = model or providers_config.get('preferred_model', 'glm-4.6:cloud')
    
    # Handle vision processing
    messages, preferred_provider, preferred_model = _handle_vision_processing(
        messages, user_message, preferred_provider, preferred_model
    )
    
    try:
        # Adjust max_tokens based on provider
        kwargs = {"timeout": 180}
        
        if preferred_provider == 'chutes':
            kwargs['max_tokens'] = 16384  # Streaming allows more tokens
        elif preferred_provider == 'openrouter':
            if ':free' in preferred_model:
                kwargs['max_tokens'] = 2048
            else:
                kwargs['max_tokens'] = 4096
        
        response_generator = ai_manager.send_message_streaming(
            preferred_provider, 
            preferred_model, 
            messages,
            **kwargs
        )
        
        full_response = ""
        for chunk in response_generator:
            if chunk:
                full_response += chunk
                yield chunk
        
        # Handle AI-initiated image generation
        if full_response and full_response.strip().startswith('/imagine'):
            result = _handle_ai_image_generation(full_response, session_id)
            if result != full_response:
                yield result
        
    except Exception as e:
        error_msg = f"AI service error: {str(e)}"
        print(f"[ERROR] Streaming response failed: {error_msg}")
        yield error_msg

def generate_ai_response(profile, user_message, interface="terminal", session_id=None):
    """Generate AI response (non-streaming)"""
    if session_id is None:
        active_session = Database.get_active_session()
        session_id = active_session['id']
    
    # Handle direct image generation
    if user_message.strip().startswith('/imagine'):
        return _handle_image_generation(user_message, session_id)
    
    # Build context and messages
    messages = _build_generation_context(profile, session_id, interface)
    
    ai_manager = get_ai_manager()
    providers_config = profile.get('providers_config', {})
    
    preferred_provider = providers_config.get('preferred_provider', 'ollama')
    preferred_model = providers_config.get('preferred_model', 'glm-4.6:cloud')
    
    # Handle vision processing
    messages, preferred_provider, preferred_model = _handle_vision_processing(
        messages, user_message, preferred_provider, preferred_model
    )
    
    try:
        # Adjust max_tokens based on provider
        kwargs = {"timeout": 180}
        
        if preferred_provider == 'chutes':
            kwargs['max_tokens'] = 4096  # Chutes non-streaming max is 8192
        elif preferred_provider == 'openrouter':
            if ':free' in preferred_model:
                kwargs['max_tokens'] = 1024  # Lower for free tier
            else:
                kwargs['max_tokens'] = 2048
        
        ai_response = ai_manager.send_message(
            preferred_provider, 
            preferred_model, 
            messages,
            **kwargs
        )
        
        # Handle AI-initiated image generation
        if ai_response and ai_response.strip().startswith('/imagine'):
            return _handle_ai_image_generation(ai_response, session_id)
        
        if ai_response:
            return ai_response
        else:
            print(f"[WARNING] AI service returned empty response")
            return "AI service failed to generate a response."
            
    except Exception as e:
        error_msg = f"AI service error: {str(e)}"
        print(f"[ERROR] AI response generation failed: {error_msg}")
        return f"Sorry, I couldn't process that image. Please try again with a different provider or check your API limits."

def auto_name_session_if_needed(session_id, active_session):
    if active_session.get('name') != 'New Chat':
        return
    
    message_count = Database.get_session_messages_count(session_id)
    
    if message_count == 10:
        conversation_summary = Database.get_session_conversation_summary(session_id, limit=15)
        
        api_keys = Database.get_api_keys()
        openrouter_key = api_keys.get('openrouter')
        
        if openrouter_key:
            name = generate_session_name_ai(conversation_summary, openrouter_key)
            if name:
                Database.rename_session(session_id, name)
                return
        
        chat_history = Database.get_chat_history(session_id, limit=5)
        for msg in chat_history:
            if msg['role'] == 'user' and len(msg['content'].strip()) > 10:
                first_msg = msg['content'].strip()[:40]
                if len(msg['content']) > 40:
                    first_msg += "..."
                Database.rename_session(session_id, first_msg)
                return
        
        fallback_name = f"Chat {session_id}"
        Database.rename_session(session_id, fallback_name)

def generate_session_name_ai(conversation_summary, api_key):
    prompt = f"""Based on this conversation, create a SHORT session title (max 6 words):

{conversation_summary}

Reply with ONLY the title, nothing else."""
    
    headers = {
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/icedeyes/yuzu-companion",
        "X-Title": "Yuzu-Session-Naming"
    }
    
    try:
        headers["Authorization"] = f"Bearer {api_key}"
        
        data = {
            "model": "tngtech/deepseek-r1t2-chimera:free",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1000,
            "temperature": 3,
            "stream": False
        }
        
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            name = result['choices'][0]['message']['content'].strip()
            name = name.replace('"', '').replace("'", "").strip()
            if len(name) > 50:
                name = name[:50] + "..."
            return name
        else:
            return None
            
    except Exception as e:
        return None

def end_session_cleanup(profile, interface="terminal", unexpected_exit=False):
    with UserContext() as context:
        active_session = Database.get_active_session()
        session_id = active_session['id']
        
        session_history = profile.get('session_history', {})
        current_session = session_history.get('current_session', {})
        start_time = current_session.get('start_time')
        
        duration = 0
        end_time = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        if start_time:
            try:
                start = datetime.fromisoformat(start_time)
                duration = (datetime.now() - start).total_seconds() / 60
            except:
                pass
        
        if unexpected_exit:
            disconnect_msg = (
                f"*{profile['display_name']} disconnected unexpectedly from {interface} "
                f"at {end_time}. Session duration: {duration:.1f} minutes*"
            )
        else:
            if duration < 1:
                time_desc = "quick"
            elif duration < 5:
                time_desc = "short"
            else:
                time_desc = f"{duration:.1f} minute"
            
            disconnect_msg = (
                f"*{profile['display_name']} disconnected from {interface} "
                f"at {end_time} after a {time_desc} session*"
            )
        
        Database.add_message('system', disconnect_msg, session_id)
        
        session_history['last_session'] = {
            'end_time': datetime.now().isoformat(),
            'end_timestamp': end_time,
            'duration_minutes': round(duration, 1),
            'message_count': current_session.get('message_count', 0),
            'interface': interface,
            'unexpected_exit': unexpected_exit
        }
        
        session_history['total_sessions'] = session_history.get('total_sessions', 0) + 1
        session_history['total_time_minutes'] = session_history.get('total_time_minutes', 0) + duration
        session_history['current_session'] = {}
        
        Database.update_profile({'session_history': session_history})
        
        if interface == "terminal":
            if unexpected_exit:
                farewell = f"Connection lost at {end_time}... system logs corrupted"
            else:
                if duration < 1:
                    farewell = f"Quick session ended at {end_time}... Come back soon!"
                elif duration < 5:
                    farewell = f"Short session ended at {end_time}... See you next time!"
                else:
                    farewell = f"Closing connection at {end_time} after {duration:.1f} minutes together... Goodbye!"
        
        return disconnect_msg

def start_session(interface="terminal"):
    with UserContext() as context:
        profile = Database.get_profile()
        active_session = Database.get_active_session()
        session_id = active_session['id']
        
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        all_sessions = Database.get_all_sessions()
        session_count = len(all_sessions)
        
        last_active = "Never"
        if len(all_sessions) > 1:
            sorted_sessions = sorted(all_sessions, key=lambda x: x.get('updated_at', ''), reverse=True)
            for session in sorted_sessions[1:]:
                if session.get('updated_at'):
                    last_active = session['updated_at']
                    break
        
        connection_msg = (
            f"*{profile['display_name']} connected to {interface} interface at {current_time}. "
            f"Last active: {last_active}. Session count: #{session_count}*"
        )
        
        Database.add_message('system', connection_msg, session_id)
        
        session_history = profile.get('session_history', {})
        session_history['current_session'] = {
            'start_time': datetime.now().isoformat(),
            'interface': interface,
            'message_count': 0,
            'start_timestamp': current_time
        }
        session_history['total_sessions'] = session_history.get('total_sessions', 0) + 1
        Database.update_profile({'session_history': session_history})
        
        return profile

def detect_important_content(message):
    important_keywords = ['love', 'hate', 'important', 'always', 'never', 'forever', 'remember']
    return any(keyword in message.lower() for keyword in important_keywords)

def should_summarize_memory(profile, user_message, session_id):
    chat_history = Database.get_chat_history(session_id=session_id)
    conversation_messages = [msg for msg in chat_history if msg['role'] in ['user', 'assistant']]
    total_conversation_count = len(conversation_messages)
    
    if total_conversation_count >= 50 and total_conversation_count % 50 == 0:
        session_memory = Database.get_session_memory(session_id)
        last_summary_count = session_memory.get('last_summary_count', 0)
        
        if total_conversation_count > last_summary_count:
            return True
    
    if detect_important_content(user_message):
        return True
    
    return False

def summarize_memory(profile, user_message, ai_reply, session_id):
    chat_history = Database.get_chat_history(session_id=session_id, limit=80)
    
    conversation_messages = [msg for msg in chat_history if msg['role'] in ['user', 'assistant']]
    current_count = len(conversation_messages)
    
    if not chat_history:
        return False
    
    conversation_text = ""
    for msg in chat_history[-100:]:
        role = "User" if msg["role"] == "user" else "AI"
        conversation_text += f"{role}: {msg['content']}\n"
    
    analysis_prompt = f"""
Write ONE paragraph summarizing the current conversation context in this session.

Recent Conversation:
{conversation_text}

Current Interaction:
User: {user_message}
AI: {ai_reply}

Write one concise paragraph (3-5 sentences) summarizing what this session is about, 
the current topics being discussed, and the general context. No lists, no bullet points, 
just a natural paragraph.
"""
    
    api_keys = Database.get_api_keys()
    openrouter_key = api_keys.get('openrouter')
    
    if not openrouter_key:
        return False
    
    context_paragraph = session_context_analysis(analysis_prompt, openrouter_key)
    
    if context_paragraph:
        memory_update = {
            "session_context": context_paragraph.strip(),
            "last_summarized": datetime.now().isoformat(),
            "last_summary_count": current_count
        }
        
        Database.update_session_memory(session_id, memory_update)
        return True
    else:
        return False

def session_context_analysis(prompt, api_key):
    headers = {
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/icedeyes12/yuzu-companion", 
        "X-Title": "Yuzu-Session-Context"
    }
    
    try:
        headers["Authorization"] = f"Bearer {api_key}"
        
        data = {
            "model": "qwen/qwen3-235b-a22b-2507",
            "messages": [
                {
                    "role": "system", 
                    "content": "You write concise, natural paragraphs summarizing conversation context. One paragraph only."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": 500,
            "temperature": 0.2,
            "stream": False
        }
        
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        else:
            return None
            
    except Exception as e:
        return None

def summarize_global_player_profile():
    """Analyze ALL conversation history across ALL sessions with optimized sampling"""
    all_sessions = Database.get_all_sessions()
    profile = Database.get_profile()
    
    # CONFIGURABLE SETTINGS
    MAX_MSGS_PER_SESSION = 2000           # Messages per session limit
    MAX_CONTEXT_CHARS = 900000           # Max characters (â‰ˆ175K tokens)
    RECENT_RATIO = 0.7                   # 70% recent messages, 30% random from older
    MIN_MSG_LENGTH = 5                   # Skip very short messages
    
    print(f"[INFO] Starting global profile analysis for {len(all_sessions)} sessions")
    print(f"[CONFIG] Max {MAX_MSGS_PER_SESSION} msgs/session, {MAX_CONTEXT_CHARS:,} max chars")
    print(f"[CONFIG] Sampling: {int(RECENT_RATIO*100)}% recent, {int((1-RECENT_RATIO)*100)}% random")
    
    # Sort sessions by date (newest first for priority)
    sorted_sessions = sorted(
        all_sessions, 
        key=lambda x: x.get('created_at', ''), 
        reverse=True
    )
    
    all_conversations = []
    total_messages_processed = 0
    total_sessions_with_data = 0
    
    for session in sorted_sessions:
        session_id = session['id']
        session_name = session.get('name', f'Session {session_id}')
        
        # Get ALL messages for this session
        all_messages = Database.get_chat_history(session_id=session_id, limit=None)
        
        if not all_messages:
            continue
            
        total_sessions_with_data += 1
        
        # Filter only user/assistant messages
        filtered_messages = [
            msg for msg in all_messages 
            if msg['role'] in ['user', 'assistant'] 
            and len(msg['content'].strip()) >= MIN_MSG_LENGTH
        ]
        
        if not filtered_messages:
            continue
            
        # Select messages with 70/30 strategy
        if len(filtered_messages) <= MAX_MSGS_PER_SESSION:
            selected_messages = filtered_messages
            selection_method = "all"
        else:
            # Calculate counts
            recent_count = int(MAX_MSGS_PER_SESSION * RECENT_RATIO)
            random_count = MAX_MSGS_PER_SESSION - recent_count
            
            # Get recent messages (newest first)
            recent_messages = filtered_messages[-recent_count:]
            
            # Get random sample from older messages
            older_messages = filtered_messages[:-recent_count]
            
            if len(older_messages) > 0 and random_count > 0:
                import random
                random_sample = random.sample(
                    older_messages, 
                    min(random_count, len(older_messages))
                )
                selected_messages = recent_messages + random_sample
                selection_method = f"recent+random ({recent_count}+{len(random_sample)})"
            else:
                selected_messages = recent_messages
                selection_method = f"recent only ({len(recent_messages)})"
        
        # Process selected messages
        session_conversations = []
        for msg in selected_messages:
            role = "User" if msg["role"] == "user" else "AI"
            content = msg['content'].strip()
            
            # Clean and truncate if too long
            if len(content) > 400:
                content = content[:400] + "..."
            
            session_conversations.append(f"{role}: {content}")
            total_messages_processed += 1
        
        if session_conversations:
            # Format session header with stats
            session_header = f"\n\n=== SESSION: {session_name} ==="
            session_header += f"\n[Total: {len(filtered_messages)} msgs | Selected: {len(selected_messages)} | Method: {selection_method}]"
            
            session_text = session_header + "\n" + "\n".join(session_conversations)
            all_conversations.append(session_text)
    
    if not all_conversations:
        print("[INFO] No conversations found for analysis")
        return False
    
    # Combine all text
    conversation_text = "".join(all_conversations)
    
    # Apply character limit
    if len(conversation_text) > MAX_CONTEXT_CHARS:
        print(f"[WARNING] Conversation text too long ({len(conversation_text):,} chars), truncating to {MAX_CONTEXT_CHARS:,}")
        # Try to keep complete sessions by removing oldest sessions first
        while len(conversation_text) > MAX_CONTEXT_CHARS and len(all_conversations) > 1:
            # Remove oldest session (first in list after reverse sort)
            removed_session = all_conversations.pop(0)
            conversation_text = "".join(all_conversations)
            print(f"[INFO] Removed oldest session, now {len(conversation_text):,} chars")
    
    print(f"[INFO] Analysis Summary:")
    print(f"  - Sessions total: {len(all_sessions)}")
    print(f"  - Sessions with data: {total_sessions_with_data}")
    print(f"  - Sessions analyzed: {len(all_conversations)}")
    print(f"  - Messages processed: {total_messages_processed}")
    print(f"  - Conversation data: {len(conversation_text):,} characters")
    print(f"  - Model utilization: ~{len(conversation_text)//4:,} tokens")
    
    # Create analysis prompt (sama seperti sebelumnya)
    analysis_prompt = f"""# PLAYER PROFILE ANALYSIS TASK

## CONVERSATION HISTORY:
Below is the complete conversation history between the User and AI across multiple sessions.

{conversation_text}

## ANALYSIS INSTRUCTIONS:
You are an expert psychologist and data analyst. Your task is to analyze the conversation history above and extract deep insights about the User.

### FOCUS AREAS:
1. **Personality Analysis**: Identify core personality traits, communication style, emotional patterns
2. **Interests & Preferences**: What does the user like/dislike? Topics they frequently discuss
3. **Behavioral Patterns**: How do they interact? Response patterns, engagement style
4. **Relationship Dynamics**: How is their relationship with the AI? Emotional connection, trust level
5. **Significant Content**: Important memories, experiences, or topics that are emotionally charged

### OUTPUT FORMAT REQUIREMENTS:
You MUST use this exact format. Do not add any commentary, explanations, or additional text.

Player Summary: [Provide a comprehensive summary of the user's personality, interests, and overall interaction patterns. Be specific and evidence-based.]

Likes: [Provide specific likes, interests, or positive preferences. Format as comma-separated list.]

Dislikes: [Provide specific dislikes, aversions, or negative preferences. Format as comma-separated list.]

Personality Traits: [Provide personality characteristics. Use descriptive adjectives.]

Important Memories: [List significant memories, experiences, or topics that were emotionally important or frequently mentioned.]

Relationship Dynamics: [Provide analysis of the relationship dynamics between User and AI. Include emotional tone, trust level, interaction patterns, and development over time.]

### CRITICAL RULES:
- Base EVERYTHING on evidence from the conversations
- Be specific and concrete - avoid vague statements
- No markdown formatting, no bullet points, no numbering
- Follow the EXACT format above - no additional sections"""
    
    api_keys = Database.get_api_keys()
    openrouter_key = api_keys.get('openrouter')
    
    if not openrouter_key:
        print("[ERROR] No OpenRouter API key found")
        return False
    
    print(f"[INFO] Sending to Server...")
    
    summary_text = global_profile_analysis(analysis_prompt, openrouter_key)
    
    if summary_text:
        print(f"[SUCCESS] Analysis received: {len(summary_text):,} chars")
        
        # Save raw analysis for debugging
        import os
        os.makedirs("debug_logs", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        debug_file = f"debug_logs/profile_summary_{timestamp}.txt"
        
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write("=== GLOBAL PROFILE ANALYSIS ===\n")
            f.write(f"Date: {timestamp}\n")
            f.write(f"Sessions: {len(all_conversations)}\n")
            f.write(f"Messages: {total_messages_processed}\n")
            f.write(f"Chars: {len(conversation_text)}\n")
            f.write("\n=== RAW ANALYSIS ===\n")
            f.write(summary_text)
        
        print(f"[DEBUG] Raw analysis saved to: {debug_file}")
        
        # Parse and update profile
        profile_update = parse_global_profile_summary(summary_text)
        profile_update['last_global_summary'] = datetime.now().isoformat()
        profile_update['sessions_analyzed'] = len(all_conversations)
        profile_update['total_messages'] = total_messages_processed
        profile_update['analysis_chars'] = len(conversation_text)
        
        # Merge with existing profile
        current_profile = Database.get_profile()
        current_memory = current_profile.get('memory', {})
        current_memory = _merge_profile_data(current_memory, profile_update)
        
        try:
            Database.update_profile({'memory': current_memory})
            
            # Success report
            print(f"\n{'='*50}")
            print(f"GLOBAL PROFILE UPDATE COMPLETE!")
            print(f"{'='*50}")
            print(f"âœ… Sessions analyzed: {len(all_conversations)}")
            print(f"âœ… Messages processed: {total_messages_processed}")
            print(f"âœ… Data volume: {len(conversation_text):,} chars")
            print(f"âœ… Player summary: {len(current_memory.get('player_summary', '')):,} chars")
            print(f"âœ… Likes identified: {len(current_memory.get('key_facts', {}).get('likes', []))}")
            print(f"âœ… Personality traits: {len(current_memory.get('key_facts', {}).get('personality_traits', []))}")
            print(f"âœ… Relationship analysis saved")
            print(f"{'='*50}")
            
            return True
            
        except Exception as e:
            print(f"[ERROR] Database update failed: {str(e)}")
            traceback.print_exc()
            return False
    
    print("[ERROR] Analysis generation failed")
    return False


def normalize_memory_item(text: str) -> str:
    """Normalize a memory item for deduplication comparison.
    
    Rules:
    - Convert to lowercase
    - Strip leading/trailing whitespace
    - Remove trailing punctuation (. , " ')
    - Collapse multiple spaces into one
    """
    text = text.strip().lower()
    text = re.sub(r'\s+', ' ', text)
    text = text.rstrip('.,"\'')
    return text


def merge_and_clean_memory(existing_list: List[str], new_items: List[str], max_size: int) -> List[str]:
    """Merge new items into an existing list with normalization-based deduplication and size limits.
    
    - Normalizes text for comparison only; preserves original text of the first occurrence.
    - Skips items whose normalized form already exists.
    - Enforces max_size by keeping the earliest items.
    """
    result = []
    seen_normalized = set()

    for item in existing_list:
        if not item or not item.strip():
            continue
        norm = normalize_memory_item(item)
        if norm and norm not in seen_normalized:
            seen_normalized.add(norm)
            result.append(item)

    for item in new_items:
        if not item or not item.strip():
            continue
        norm = normalize_memory_item(item)
        if norm and norm not in seen_normalized:
            seen_normalized.add(norm)
            result.append(item)

    return result[:max_size]


# Maximum list sizes for each key_facts category
_MEMORY_LIST_LIMITS = {
    'likes': 30,
    'dislikes': 30,
    'personality_traits': 15,
    'important_memories': 20,
}


def _merge_profile_data(existing_memory: Dict, new_data: Dict) -> Dict:
    """Smart merge of existing profile data with new analysis"""
    if not existing_memory:
        return new_data
    
    result = existing_memory.copy()
    
    # Merge player summary - keep newer if significantly different
    if 'player_summary' in new_data and new_data['player_summary']:
        existing_summary = result.get('player_summary', '')
        new_summary = new_data['player_summary']
        
        # Keep the more detailed summary
        if len(new_summary) > len(existing_summary) * 1.5:  # New summary is 50% longer
            result['player_summary'] = new_summary
            print(f"[INFO] Updated player summary (new: {len(new_summary)} chars, old: {len(existing_summary)} chars)")
    
    # Merge relationship dynamics
    if 'relationship_dynamics' in new_data and new_data['relationship_dynamics']:
        result['relationship_dynamics'] = new_data['relationship_dynamics']
    
    # Merge key facts with normalization, deduplication, and size limits
    if 'key_facts' in new_data:
        if 'key_facts' not in result:
            result['key_facts'] = {
                'likes': [],
                'dislikes': [],
                'personality_traits': [],
                'important_memories': []
            }
        
        for category in ['likes', 'dislikes', 'personality_traits', 'important_memories']:
            existing_items = result['key_facts'].get(category, [])
            new_items = new_data['key_facts'].get(category, [])
            max_size = _MEMORY_LIST_LIMITS[category]
            
            existing_normalized = {normalize_memory_item(i) for i in existing_items if i and i.strip()}
            added = sum(1 for i in new_items if i and i.strip() and normalize_memory_item(i) not in existing_normalized)
            
            merged = merge_and_clean_memory(existing_items, new_items, max_size)
            result['key_facts'][category] = merged
            
            if added > 0:
                print(f"[INFO] Added {added} new items to {category}")
    
    # Update metadata
    result['last_global_summary'] = new_data.get('last_global_summary', '')
    result['sessions_analyzed'] = new_data.get('sessions_analyzed', 0)
    
    return result


def global_profile_analysis(prompt: str, api_key: str) -> Optional[str]:
    """Analyze conversation history using GLM-4.7 with optimal settings"""
    headers = {
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/icedeyes12/yuzu-companion", 
        "X-Title": "Yuzu-Global-Profile",
        "Authorization": f"Bearer {api_key}"
    }
    
    try:
        
        model = "qwen/qwen3-235b-a22b-2507"
        
        # Optimize for long context analysis
        data = {
            "model": model,
            "messages": [
                {
                    "role": "system", 
                    "content": """You are an expert psychologist and data analyst specializing in conversation analysis. 
                    Your task is to extract deep, meaningful insights from conversation history.
                    
                    ANALYSIS APPROACH:
                    1. Read and comprehend the ENTIRE conversation history
                    2. Identify patterns, themes, and significant moments
                    3. Extract evidence-based insights about personality, preferences, and relationship dynamics
                    4. Be specific, concrete, and thorough
                    5. Follow the output format EXACTLY as specified
                    
                    Your analysis should be comprehensive, insightful, and directly based on the conversation evidence."""
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "temperature": 0.2,  # Low temperature for consistent analysis
            "max_tokens": 4000,  
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.1,
            "stream": False
        }
        
        print(f"[DEBUG] Using model: {model}")
        print(f"[DEBUG] Prompt tokens estimate: ~{len(prompt) // 4}")
        print(f"[DEBUG] Max response tokens: {data['max_tokens']}")
        
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=300  # 5 minute timeout for large analysis
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            # Periksa apakah response terpotong
            if 'finish_reason' in result['choices'][0]:
                finish_reason = result['choices'][0]['finish_reason']
                if finish_reason == 'length':
                    print("[WARNING] Response may have been truncated due to token limit")
                elif finish_reason == 'stop':
                    print("[INFO] Response completed normally")
            
            print(f"[SUCCESS] Analysis complete: {len(content):,} characters")
            return content
            
        else:
            error_msg = f"OpenRouter API error: {response.status_code}"
            
            try:
                error_data = response.json()
                error_detail = error_data.get('error', {}).get('message', 'Unknown')
                error_msg += f" - {error_detail}"
                
                print(f"[ERROR] {error_msg}")
                
                # Handle specific errors
                if "insufficient_quota" in error_detail.lower():
                    print("[ERROR] Insufficient API quota")
                    return _try_free_model(prompt, api_key)
                elif "model not found" in error_detail.lower():
                    print("[WARNING] GLM-4.7 not available, trying alternatives...")
                    return _try_alternative_models(prompt, api_key)
                    
            except:
                error_msg += f" - {response.text[:200]}"
                print(f"[ERROR] {error_msg}")
            
            return None
            
    except requests.exceptions.Timeout:
        print("[ERROR] Request timeout - analysis took too long")
        return None
    except Exception as e:
        print(f"[ERROR] Analysis failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def _try_alternative_models(prompt: str, api_key: str) -> Optional[str]:
    """Try alternative models if GLM-4.7 fails"""
    alternatives = [
        ("z-ai/glm-4.6", 8000),  # GLM-4.6
        ("qwen/qwen3-235b-a22b-2507", 6000),  # Qwen 3 235B
        ("deepseek/deepseek-chat-v3.1", 6000),  # DeepSeek V3.1
        ("tngtech/deepseek-r1t2-chimera", 4000),  # Chimera
        ("google/gemini-2.0-flash-exp:free", 4000),  # Gemini Flash (free)
    ]
    
    headers = {
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/icedeyes12/yuzu-companion", 
        "X-Title": "Yuzu-Global-Profile",
        "Authorization": f"Bearer {api_key}"
    }
    
    for model, max_tokens in alternatives:
        print(f"[INFO] Trying alternative model: {model}")
        
        try:
            # Kurangi prompt untuk model dengan context lebih kecil
            if model.endswith(":free") or "flash" in model.lower():
                # Model free/light perlu prompt lebih pendek
                shortened_prompt = prompt[:20000] + "\n\n...[prompt truncated for lighter model]"
                actual_prompt = shortened_prompt
            else:
                actual_prompt = prompt
            
            data = {
                "model": model,
                "messages": [
                    {
                        "role": "system", 
                        "content": "You are a conversation analyst. Extract comprehensive insights from the conversation history."
                    },
                    {
                        "role": "user", 
                        "content": actual_prompt
                    }
                ],
                "temperature": 0.2,
                "max_tokens": max_tokens,
                "top_p": 0.9,
                "stream": False
            }
            
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=180
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                print(f"[SUCCESS] Got response from {model}: {len(content):,} chars")
                return content
            else:
                print(f"[WARNING] {model} failed: {response.status_code}")
                continue
                
        except Exception as e:
            print(f"[WARNING] Error with {model}: {str(e)}")
            continue
    
    print("[ERROR] All alternative models failed")
    return None


def _try_free_model(prompt: str, api_key: str) -> Optional[str]:
    """Try free model if quota exhausted"""
    free_models = [
        "google/gemini-2.0-flash-exp:free",
        "deepseek/deepseek-chat-v3.1:free",
        "qwen/qwen3-235b-a22b:free",
    ]
    
    headers = {
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/icedeyes12/yuzu-companion", 
        "X-Title": "Yuzu-Global-Profile",
        "Authorization": f"Bearer {api_key}"
    }
    
    for model in free_models:
        print(f"[INFO] Trying free model: {model}")
        
        try:
            # Potong prompt secara signifikan untuk model free
            shortened_prompt = prompt[:15000] + "\n\n...[analysis limited due to free tier constraints]"
            
            data = {
                "model": model,
                "messages": [
                    {
                        "role": "system", 
                        "content": "Extract key insights from conversation history. Focus on most important patterns."
                    },
                    {
                        "role": "user", 
                        "content": shortened_prompt
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 2000,  # Free models have lower limits
                "top_p": 0.9,
                "stream": False
            }
            
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                print(f"[SUCCESS] Free model response: {len(content):,} chars")
                return content
                
        except Exception as e:
            print(f"[WARNING] Free model {model} error: {str(e)}")
            continue
    
    print("[ERROR] All free models failed")
    return None


def parse_global_profile_summary(summary_text: str) -> Dict:
    """Parse the global profile summary text into structured data"""
    profile_data = {
        "player_summary": "",
        "key_facts": {
            "likes": [],
            "dislikes": [], 
            "personality_traits": [],
            "important_memories": []
        },
        "relationship_dynamics": "",
        "last_updated": datetime.now().isoformat()
    }
    
    # Clean the text
    cleaned_text = summary_text.replace('\r\n', '\n').replace('\r', '\n')
    
    # Normalize section headers
    section_patterns = {
        'Player Summary:': 'player_summary',
        'Player Summary': 'player_summary',
        'Summary:': 'player_summary',
        'Summary': 'player_summary',
        
        'Likes:': 'likes',
        'Likes': 'likes',
        'Interests:': 'likes',
        'Interests': 'likes',
        
        'Dislikes:': 'dislikes',
        'Dislikes': 'dislikes',
        'Aversions:': 'dislikes',
        
        'Personality Traits:': 'personality_traits',
        'Personality Traits': 'personality_traits',
        'Traits:': 'personality_traits',
        'Personality:': 'personality_traits',
        
        'Important Memories:': 'important_memories',
        'Important Memories': 'important_memories',
        'Memories:': 'important_memories',
        'Key Memories:': 'important_memories',
        
        'Relationship Dynamics:': 'relationship_dynamics',
        'Relationship Dynamics': 'relationship_dynamics',
        'Relationship:': 'relationship_dynamics',
        'Dynamics:': 'relationship_dynamics'
    }
    
    lines = cleaned_text.split('\n')
    current_section = None
    buffer = []
    
    def save_current_section():
        if current_section and buffer:
            content = ' '.join(buffer).strip()
            if current_section == 'player_summary':
                profile_data["player_summary"] = content
            elif current_section == 'relationship_dynamics':
                profile_data["relationship_dynamics"] = content
            elif current_section in ['likes', 'dislikes', 'personality_traits', 'important_memories']:
                # Parse comma-separated lists
                items = []
                for item in content.split(','):
                    item_clean = item.strip()
                    if item_clean:
                        items.append(item_clean)
                profile_data["key_facts"][current_section] = items
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Check if this line starts a new section
        section_found = False
        for pattern, section_key in section_patterns.items():
            if line.startswith(pattern):
                # Save previous section
                save_current_section()
                
                # Start new section
                current_section = section_key
                buffer = []
                
                # Remove the pattern from the line
                remaining = line[len(pattern):].strip()
                if remaining:
                    buffer.append(remaining)
                
                section_found = True
                break
        
        if not section_found and current_section:
            # Continue current section
            buffer.append(line)
    
    # Save the last section
    save_current_section()
    
    # Clean up the data
    for section in ['player_summary', 'relationship_dynamics']:
        if profile_data[section]:
            # Remove any trailing punctuation
            profile_data[section] = profile_data[section].strip()
            if profile_data[section].endswith('.'):
                profile_data[section] = profile_data[section][:-1]
    
    # Clean lists
    for key in ['likes', 'dislikes', 'personality_traits', 'important_memories']:
        if profile_data["key_facts"][key]:
            # Remove duplicates and empty items
            unique_items = []
            seen = set()
            for item in profile_data["key_facts"][key]:
                if item and item not in seen:
                    seen.add(item)
                    unique_items.append(item)
            profile_data["key_facts"][key] = unique_items
    
    print(f"[DEBUG] Parsed profile: player_summary={len(profile_data['player_summary'])} chars, "
          f"likes={len(profile_data['key_facts']['likes'])}, "
          f"personality_traits={len(profile_data['key_facts']['personality_traits'])}")
    
    return profile_data


def save_section_content(profile_data, section_key, content):
    if section_key == 'player_summary':
        if not profile_data["player_summary"]:
            profile_data["player_summary"] = content
        else:
            profile_data["player_summary"] += " " + content
            
    elif section_key == 'relationship_dynamics':
        if not profile_data["relationship_dynamics"]:
            profile_data["relationship_dynamics"] = content
        else:
            profile_data["relationship_dynamics"] += " " + content
            
    elif section_key in ['likes', 'dislikes', 'personality_traits', 'important_memories']:
        items = [item.strip() for item in content.split(',') if item.strip()]
        profile_data["key_facts"][section_key].extend(items)


def get_available_providers():
    ai_manager = get_ai_manager()
    return ai_manager.get_available_providers()


def get_all_models():
    ai_manager = get_ai_manager()
    return ai_manager.get_all_models()


def set_preferred_provider(provider_name, model_name=None):
    profile = Database.get_profile()
    providers_config = profile.get('providers_config', {})
    
    providers_config['preferred_provider'] = provider_name
    if model_name:
        providers_config['preferred_model'] = model_name
    
    Database.update_profile({'providers_config': providers_config})
    return f"Preferred provider set to: {provider_name}" + (f" with model: {model_name}" if model_name else "")


def get_provider_models(provider_name):
    ai_manager = get_ai_manager()
    return ai_manager.get_provider_models(provider_name)


def get_vision_capabilities():
    from tools import multimodal_tools
    
    capabilities = {
        'has_vision': False,
        'vision_provider': None,
        'vision_model': None,
        'has_image_generation': False,
        'image_generation_provider': None
    }
    
    vision_provider, vision_model = multimodal_tools.get_best_vision_provider()
    if vision_provider:
        capabilities['has_vision'] = True
        capabilities['vision_provider'] = vision_provider
        capabilities['vision_model'] = vision_model
    
    api_keys = Database.get_api_keys()
    if 'openrouter' in api_keys:
        capabilities['has_image_generation'] = True
        capabilities['image_generation_provider'] = 'openrouter'
    
    return capabilities


def check_glm47_capabilities():
    """Check GLM-4.7 capabilities and pricing"""
    print("=== GLM-4.7 Capabilities ===")
    print("Context window: 202,800 tokens")
    print("Max output: 65,536 tokens")
    print("\n=== OpenRouter Pricing ===")
    print("z-ai/glm-4.7: ~$0.50 per 1M input tokens")
    print("z-ai/glm-4.6: ~$0.20 per 1M input tokens")
    print("\n=== Recommendations ===")
    print("1. Use GLM-4.7 for comprehensive analysis")
    print("2. Monitor token usage in OpenRouter dashboard")
    print("3. Consider batch analysis for large histories")
    print("4. Cache results to avoid repeated analysis")
    
    # Estimate cost
    api_keys = Database.get_api_keys()
    openrouter_key = api_keys.get('openrouter')
    
    if openrouter_key:
        # Check usage via OpenRouter API
        headers = {"Authorization": f"Bearer {openrouter_key}"}
        try:
            response = requests.get(
                "https://openrouter.ai/api/v1/auth/key",
                headers=headers,
                timeout=30
            )
            if response.status_code == 200:
                key_info = response.json()
                print(f"\n=== API Key Info ===")
                print(f"Label: {key_info.get('label', 'N/A')}")
                print(f"Usage: {key_info.get('usage', 'N/A')}")
                print(f"Limits: {key_info.get('limits', 'N/A')}")
        except:
            print("\n[INFO] Could not fetch API key details")


def test_glm47_profile():
    """Test GLM-4.7 for profile analysis"""
    print("=== Testing GLM-4.7 Profile Analysis ===")
    
    api_keys = Database.get_api_keys()
    openrouter_key = api_keys.get('openrouter')
    
    if not openrouter_key:
        print("[ERROR] No OpenRouter API key!")
        return False
    
    # Test dengan prompt sederhana
    test_prompt = """# PLAYER PROFILE ANALYSIS TASK

## CONVERSATION HISTORY:
Session 1 - User: Hello, I really enjoy programming in Python
Session 1 - AI: That's great! Python is a wonderful language
Session 1 - User: Yes, I especially like data analysis with pandas
Session 2 - User: I love listening to jazz while working
Session 2 - AI: Jazz is perfect for focus! Any favorite artists?
Session 2 - User: Miles Davis and John Coltrane are my favorites

## ANALYSIS INSTRUCTIONS:
Analyze the conversation history and extract insights about the User.

### OUTPUT FORMAT:
Player Summary: [4-6 sentence summary]

Likes: [comma-separated list]

Dislikes: [comma-separated list]

Personality Traits: [comma-separated list]

Important Memories: [comma-separated list]

Relationship Dynamics: [3-4 sentence analysis]"""
    
    print("[INFO] Testing with simple prompt...")
    result = global_profile_analysis(test_prompt, openrouter_key)
    
    if result:
        print(f"\n[SUCCESS] GLM-4.7 Test Response:\n{result}")
        
        parsed = parse_global_profile_summary(result)
        print(f"\n[SUCCESS] Parsed Result:")
        print(f"Player Summary: {parsed['player_summary'][:200]}...")
        print(f"Likes: {parsed['key_facts']['likes']}")
        print(f"Personality Traits: {parsed['key_facts']['personality_traits']}")
        
        # Simpan test result
        with open("debug_logs/glm47_test_result.txt", "w", encoding="utf-8") as f:
            f.write("=== GLM-4.7 Test Result ===\n")
            f.write(result)
            f.write("\n\n=== Parsed Data ===\n")
            f.write(json.dumps(parsed, indent=2, ensure_ascii=False))
        
        return True
    else:
        print("[ERROR] No response from GLM-4.7")
        return False


def batch_global_analysis(max_sessions=50):
    """Run global analysis with batch processing"""
    print(f"=== Batch Global Analysis (max {max_sessions} sessions) ===")
    
    # Get all sessions
    all_sessions = Database.get_all_sessions()
    
    if len(all_sessions) > max_sessions:
        print(f"[INFO] Too many sessions ({len(all_sessions)}), limiting to {max_sessions}")
        all_sessions = all_sessions[:max_sessions]
    
    # Create a modified version that accepts session limit
    # For now, just use the standard function
    result = summarize_global_player_profile()
    
    if result:
        print("[SUCCESS] Batch analysis completed")
    else:
        print("[ERROR] Batch analysis failed")
    
    return result


def incremental_profile_update():
    """Update profile incrementally - only analyze new sessions"""
    profile = Database.get_profile()
    memory = profile.get('memory', {})
    
    last_update = memory.get('last_global_summary')
    if last_update:
        try:
            last_date = datetime.fromisoformat(last_update.replace('Z', '+00:00'))
            print(f"Last profile update: {last_date}")
            
            # Get sessions after last update
            all_sessions = Database.get_all_sessions()
            new_sessions = []
            
            for session in all_sessions:
                session_updated = session.get('updated_at')
                if session_updated:
                    try:
                        session_date = datetime.fromisoformat(session_updated.replace('Z', '+00:00'))
                        if session_date > last_date:
                            new_sessions.append(session)
                    except:
                        continue
            
            print(f"Found {len(new_sessions)} new sessions since last update")
            
            if new_sessions:
                # For now, run full analysis
                print("[INFO] Running full analysis with new sessions...")
                return summarize_global_player_profile()
            else:
                print("[INFO] No new sessions to analyze")
                return False
                
        except Exception as e:
            print(f"[ERROR] Error parsing dates: {str(e)}")
            # Run full analysis as fallback
            return summarize_global_player_profile()
    else:
        print("[INFO] No previous profile analysis found, running full analysis")
        return summarize_global_player_profile()